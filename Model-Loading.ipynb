{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15f1407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anil_\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8a362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  #image_np = np.array(Image.open(image_path))\n",
    "  image = np.asarray(Image.open(image))\n",
    "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "  input_tensor = tf.convert_to_tensor(image)/255\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "  # Run inference\n",
    "  model_fn = model.signatures['serving_default']\n",
    "  output_dict = model_fn(input_tensor)\n",
    "  return (output_dict['dense_1'].numpy()*100).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314edc07",
   "metadata": {},
   "source": [
    "##Option 1 - Reading as a Tensor Flow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a3746d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Image 0 - 0 % of Apple\n",
      "In Image 0 - 0 % of Avocado\n",
      "In Image 0 - 0 % of Banana\n",
      "In Image 0 - 0 % of Cherry\n",
      "In Image 0 - 0 % of Cocos\n",
      "In Image 0 - 0 % of Kiwi\n",
      "In Image 0 - 0 % of Lemon\n",
      "In Image 0 - 99 % of Mango\n",
      "In Image 0 - 0 % of Orange\n"
     ]
    }
   ],
   "source": [
    "TEST_IMAGE_PATHS = '../input/fruits-360_dataset/fruits-360/Test/Kiwi/r_64_100.jpg'\n",
    "TEST_IMAGE_PATHS = '../input/fruits-360_dataset/fruits-360/Test/Mango/0_100.jpg'\n",
    "\n",
    "detection_model = tf.saved_model.load('saved_model/my_model')\n",
    "fruit_names=[\"Apple\",\"Avocado\",\"Banana\",\"Cherry\",\"Cocos\",\"Kiwi\",\"Lemon\",\"Mango\",\"Orange\"]\n",
    "\n",
    "\n",
    "predict = run_inference_for_single_image(detection_model, TEST_IMAGE_PATHS)\n",
    "\n",
    "for index1, cell in enumerate(predict):\n",
    "    for index, value in enumerate(fruit_names):\n",
    "        print(\"In Image\", index1,\"-\", cell[index], \"% of\", value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab29804",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Option 2 - Reading into CV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5483e2d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\net.cpp:79: error: (-215:Assertion failed) !empty() in function 'cv::dnn::dnn4_v20220524::Net::forward'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1596/3539581599.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Runs a forward pass to compute the net output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mnetworkOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensorflowNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetworkOutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\net.cpp:79: error: (-215:Assertion failed) !empty() in function 'cv::dnn::dnn4_v20220524::Net::forward'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "TEST_IMAGE_PATH1 = '../input/fruits-360_dataset/fruits-360/Test/Kiwi/r_64_100.jpg'\n",
    "TEST_IMAGE_PATH2 = '../input/fruits-360_dataset/fruits-360/Test/Mango/0_100.jpg'\n",
    "TEST_IMAGE_PATH3 = '../input/fruits-360_dataset/fruits-360/Test/Avocado/4_100.jpg'\n",
    "# Load a model imported from Tensorflow\n",
    "tensorflowNet = cv2.dnn.readNetFromTensorflow('simple_frozen_graph.pb','tf_label_map.pbtxt')\n",
    "#tensorflowNet = cv2.dnn.readNetFromTensorflow('saved_model.pb','tf_label_map.pbtxt')\n",
    "#tensorflowNet = cv2.dnn.readNetFromTensorflow('simple_frozen_graph.pb','tf_label_map.pbtxt')\n",
    "#tensorflowNet = cv2.dnn.readNetFromTensorflow('simple_frozen_graph.pb','tf_label_map.pbtxt')\n",
    "\n",
    "# Input image\n",
    "img = cv2.imread(TEST_IMAGE_PATH1)\n",
    "rows, cols, channels = img.shape\n",
    " \n",
    "# Use the given image as input, which needs to be blob(s).\n",
    "tensorflowNet.setInput(cv2.dnn.blobFromImage(img, size=(100, 100), swapRB=True, crop=False))\n",
    " \n",
    "# Runs a forward pass to compute the net output\n",
    "networkOutput = tensorflowNet.forward()\n",
    "\n",
    "print(networkOutput)\n",
    " \n",
    "# Loop on the outputs\n",
    "for detection in networkOutput[0,0]:\n",
    "    \n",
    "    score = float(detection[2])\n",
    "    if score > 0.2:\n",
    "     \n",
    "        left = detection[3] * cols\n",
    "        top = detection[4] * rows\n",
    "        right = detection[5] * cols\n",
    "        bottom = detection[6] * rows\n",
    " \n",
    "        #draw a red rectangle around detected objects\n",
    "        cv2.rectangle(img, (int(left), int(top)), (int(right), int(bottom)), (0, 0, 255), thickness=2)\n",
    " \n",
    "# Show the image with a rectagle surrounding the detected objects \n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
